{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WNWHEPvCuqRU",
        "NaIoXi9stykk",
        "Re0FlAY-t7M2",
        "zhfJjs_AuCl9",
        "8IWjgFYTuKR4",
        "KcnXSCK7uTdf",
        "jwFgDFIvudIU",
        "XnHTGi4NuisC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. **Install PySpark**"
      ],
      "metadata": {
        "id": "WNWHEPvCuqRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEoMytT_uzW-",
        "outputId": "b3241211-1b2a-435e-ed6f-5a5b2a23cd7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=fb784e791a1e709030fe49dbaa69b2785fdaa74efcf0671192b7504be4bba469\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Create a Pandas DataFrame**:\n"
      ],
      "metadata": {
        "id": "NaIoXi9stykk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54], ['Ann', 34]]\n",
        "pandasDF = pd.DataFrame(data, columns=['Name', 'Age'])\n",
        "print(pandasDF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZOWNzmFt1-I",
        "outputId": "d0f187e5-3307-43ef-b26e-dff66d6636bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Name  Age\n",
            "0   Scott   50\n",
            "1    Jeff   45\n",
            "2  Thomas   54\n",
            "3     Ann   34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Creates a Pandas DataFrame with the provided data and prints it.\n"
      ],
      "metadata": {
        "id": "D168G6Not5_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Initialize Spark session**:"
      ],
      "metadata": {
        "id": "Re0FlAY-t7M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "15q1JQHtt-UR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   Initializes a Spark session."
      ],
      "metadata": {
        "id": "FfeCRK5FuBXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Convert Pandas DataFrame to Spark DataFrame**:\n"
      ],
      "metadata": {
        "id": "zhfJjs_AuCl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparkDF = spark.createDataFrame(pandasDF)\n",
        "\n",
        "sparkDF.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgKqgMRZuF3x",
        "outputId": "76be8b28-d99b-4e60-fdd3-aa2f7db77a93"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sparkDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O-UNfXovRtT",
        "outputId": "6854d854-b343-4a1a-ab3b-dfbfa9c5e285"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+\n",
            "|  Name|Age|\n",
            "+------+---+\n",
            "| Scott| 50|\n",
            "|  Jeff| 45|\n",
            "|Thomas| 54|\n",
            "|   Ann| 34|\n",
            "+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Converts the Pandas DataFrame to a Spark DataFrame and prints its schema and content."
      ],
      "metadata": {
        "id": "ER_S3FF9uIzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Define a custom schema**:\n"
      ],
      "metadata": {
        "id": "8IWjgFYTuKR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "mySchema = StructType([\n",
        "    StructField(\"First Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "_37PS3UyuOIM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Defines a custom schema for the DataFrame.\n"
      ],
      "metadata": {
        "id": "ehrT93u3uRvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Apply the custom schema**:\n"
      ],
      "metadata": {
        "id": "KcnXSCK7uTdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparkDF2 = spark.createDataFrame(pandasDF, schema=mySchema)\n",
        "\n",
        "sparkDF2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuCHLr0zuWOc",
        "outputId": "0aab05d7-c556-4fcf-8e4f-6d1e230f6ee6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- First Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sparkDF2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nsi7lbGvfKq",
        "outputId": "210f9227-a133-497e-b25e-141023c1695b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---+\n",
            "|First Name|Age|\n",
            "+----------+---+\n",
            "|     Scott| 50|\n",
            "|      Jeff| 45|\n",
            "|    Thomas| 54|\n",
            "|       Ann| 34|\n",
            "+----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Creates a new Spark DataFrame with the custom schema and prints its schema and content.\n"
      ],
      "metadata": {
        "id": "dpzpZzZFuYi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. **Configure Spark to use Apache Arrow**:\n"
      ],
      "metadata": {
        "id": "jwFgDFIvudIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")\n",
        "pandasDF2 = sparkDF2.select(\"*\").toPandas()\n",
        "print(pandasDF2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAF1cr7MufZt",
        "outputId": "73bc492e-8512-4bf0-d64b-7bc518749287"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  First Name  Age\n",
            "0      Scott   50\n",
            "1       Jeff   45\n",
            "2     Thomas   54\n",
            "3        Ann   34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Configures Spark to use Apache Arrow for faster conversion between Pandas and Spark DataFrames and converts the Spark DataFrame back to a Pandas DataFrame.\n"
      ],
      "metadata": {
        "id": "UpoColVbuhjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. **Verify Spark configurations for Apache Arrow**:\n"
      ],
      "metadata": {
        "id": "XnHTGi4NuisC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")`**:\n",
        "   - **Purpose**: Enables the use of Apache Arrow in PySpark.\n",
        "   - **Explanation**: When this configuration is set to `true`, PySpark uses Apache Arrow to optimize the conversion between Spark DataFrames and Pandas DataFrames. Arrow provides a more efficient in-memory format that can speed up the conversion process significantly.\n",
        "\n",
        "2. **`spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")`**:\n",
        "   - **Purpose**: Enables fallback to non-Arrow implementation if Arrow-based conversion fails.\n",
        "   - **Explanation**: If there is an issue with the Arrow-based conversion (e.g., due to incompatibility or a specific edge case), Spark will fall back to the traditional conversion method. This ensures that the conversion process is robust and doesn't fail abruptly.\n",
        "\n",
        "### Why Use Apache Arrow?\n",
        "\n",
        "- **Performance**: Arrow optimizes the conversion process, reducing the time required to convert large datasets between Spark and Pandas. This is especially beneficial when dealing with big data, where conversion overhead can be significant.\n",
        "- **Memory Efficiency**: Arrow's columnar memory layout is designed for efficient memory use, which can help reduce the memory footprint during conversions.\n",
        "- **Cross-Language Support**: Arrow provides a standardized memory format that can be used across different languages (e.g., Python, Java, R), making it easier to share data between different parts of a data processing pipeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "bqaxX8fdwUNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arrow_enabled = spark.conf.get(\"spark.sql.execution.arrow.enabled\")\n",
        "print(\"Apache Arrow Enabled:\", arrow_enabled)\n",
        "\n",
        "arrow_fallback_enabled = spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\n",
        "print(\"Apache Arrow Fallback Enabled:\", arrow_fallback_enabled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFds1pA2ukuh",
        "outputId": "e7d2d1db-cfdc-478b-8e8a-e2d934f0fe0f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apache Arrow Enabled: true\n",
            "Apache Arrow Fallback Enabled: true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Retrieves and prints the Spark configurations to verify if Apache Arrow is enabled.\n"
      ],
      "metadata": {
        "id": "ymPMljkduoKZ"
      }
    }
  ]
}