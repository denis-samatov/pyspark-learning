{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Initializing Spark Session**:\n"
      ],
      "metadata": {
        "id": "hxaSDzW90h8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQORWvY60lCF",
        "outputId": "1234f8ad-e4be-4597-b4de-63f4d1930e6d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=2d8859aa10edd1fb6bf41d0691c76d30c505bc21cb2ab3e0fa02166724cd69e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from pyspark.sql.functions import col, lit, create_map\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
      ],
      "metadata": {
        "id": "iCkQTy1-1Wyr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - Imports necessary PySpark libraries.\n",
        "   - Initializes a Spark session with the application name 'SparkByExamples.com'.\n"
      ],
      "metadata": {
        "id": "ZUZfU8hr1YZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. **Defining Sample Data and Schema**:\n"
      ],
      "metadata": {
        "id": "K7n1WeYG1Z9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"36636\", \"Finance\", 3000, \"USA\"),\n",
        "    (\"40288\", \"Finance\", 5000, \"IND\"),\n",
        "    (\"42114\", \"Sales\", 3900, \"USA\"),\n",
        "    (\"39192\", \"Marketing\", 2500, \"CAN\"),\n",
        "    (\"34534\", \"Sales\", 6500, \"USA\")\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField('id', StringType(), True),\n",
        "    StructField('dept', StringType(), True),\n",
        "    StructField('salary', IntegerType(), True),\n",
        "    StructField('location', StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "gePKUARx1YJG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Defines sample data as a list of tuples, where each tuple represents a row in the DataFrame.\n",
        "- Defines a schema with four fields: `id`, `dept`, `salary`, and `location`.\n"
      ],
      "metadata": {
        "id": "m0RNk2_71ePy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. **Creating DataFrame**:\n"
      ],
      "metadata": {
        "id": "m4WQlHya1fnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data=data, schema=schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKRvI5h61hxY",
        "outputId": "28f5f534-d20e-4560-973d-cef2b25293b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- dept: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            "\n",
            "+-----+---------+------+--------+\n",
            "|id   |dept     |salary|location|\n",
            "+-----+---------+------+--------+\n",
            "|36636|Finance  |3000  |USA     |\n",
            "|40288|Finance  |5000  |IND     |\n",
            "|42114|Sales    |3900  |USA     |\n",
            "|39192|Marketing|2500  |CAN     |\n",
            "|34534|Sales    |6500  |USA     |\n",
            "+-----+---------+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Creates a DataFrame from the sample data and schema.\n",
        "- Prints the schema of the DataFrame.\n",
        "- Displays the content of the DataFrame without truncating the output.\n"
      ],
      "metadata": {
        "id": "TjFfM1va1jFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. **Converting Columns to a Map and Dropping Original Columns**:\n"
      ],
      "metadata": {
        "id": "WaEEyj6G1kVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"propertiesMap\", create_map(\n",
        "    lit(\"salary\"), col(\"salary\"),\n",
        "    lit(\"location\"), col(\"location\")\n",
        ")).drop(\"salary\", \"location\")"
      ],
      "metadata": {
        "id": "3mh8qB_y1l1I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Uses `withColumn` and `create_map` to create a new column `propertiesMap` that maps the values of `salary` and `location`.\n",
        "- Uses `lit` to create literal expressions for the map keys (`\"salary\"` and `\"location\"`).\n",
        "- Uses `col` to refer to the values of `salary` and `location` columns.\n",
        "- Drops the original `salary` and `location` columns using `drop`.\n"
      ],
      "metadata": {
        "id": "C2fDSr6g1n1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. **Displaying the Updated DataFrame**:\n"
      ],
      "metadata": {
        "id": "Y0LcXEZH1pcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u--AHXqp1rVf",
        "outputId": "2f8a8ab4-c441-4242-c200-c109aa6ab53b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- dept: string (nullable = true)\n",
            " |-- propertiesMap: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+-----+---------+---------------------------------+\n",
            "|id   |dept     |propertiesMap                    |\n",
            "+-----+---------+---------------------------------+\n",
            "|36636|Finance  |{salary -> 3000, location -> USA}|\n",
            "|40288|Finance  |{salary -> 5000, location -> IND}|\n",
            "|42114|Sales    |{salary -> 3900, location -> USA}|\n",
            "|39192|Marketing|{salary -> 2500, location -> CAN}|\n",
            "|34534|Sales    |{salary -> 6500, location -> USA}|\n",
            "+-----+---------+---------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Prints the schema of the updated DataFrame to show the new `propertiesMap` column.\n",
        "  - Displays the content of the updated DataFrame without truncating the output.\n"
      ],
      "metadata": {
        "id": "g-pj2qOh1s_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Points\n",
        "\n",
        "- **Creating DataFrame**: Demonstrates how to create a DataFrame with a given schema and data.\n",
        "- **Converting Columns to Map**: Shows how to convert specific columns to a map type and drop the original columns.\n",
        "- **Using Functions**: Utilizes `col`, `lit`, and `create_map` functions from PySpark to manipulate the DataFrame.\n"
      ],
      "metadata": {
        "id": "fuzK2Hk81wsn"
      }
    }
  ]
}