{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. **Install PySpark**"
      ],
      "metadata": {
        "id": "TJKjcexez9Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc51oRbw0MkQ",
        "outputId": "3186534e-69df-4cd4-b36a-0edac1b522ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=3f247cb9aa5f73c88d5d05a628ef572cb93f06fe02a7788b44f6b8e5054bf65b\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Importing Libraries and Initializing Spark Session**:"
      ],
      "metadata": {
        "id": "vcvO1oWT76JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()"
      ],
      "metadata": {
        "id": "EgmZVgBp7-Ki"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - Imports necessary PySpark libraries.\n",
        "   - Initializes a Spark session with the application name 'SparkByExamples.com'.\n"
      ],
      "metadata": {
        "id": "H0v_tZW68AUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Defining Array Column Type**:\n"
      ],
      "metadata": {
        "id": "-im6uoLz8Buc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arrayCol = ArrayType(StringType(), False)"
      ],
      "metadata": {
        "id": "r3eK-QaF8EHF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - Defines an array column type with elements of `StringType` and specifies that the array cannot contain `null` values.\n"
      ],
      "metadata": {
        "id": "OCTsHAC58GY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Defining Sample Data and Schema**:\n"
      ],
      "metadata": {
        "id": "UdWs0jji8HdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"James,,Smith\", [\"Java\", \"Scala\", \"C++\"], [\"Spark\", \"Java\"], \"OH\", \"CA\"),\n",
        "    (\"Michael,Rose,\", [\"Spark\", \"Java\", \"C++\"], [\"Spark\", \"Java\"], \"NY\", \"NJ\"),\n",
        "    (\"Robert,,Williams\", [\"CSharp\", \"VB\"], [\"Spark\", \"Python\"], \"UT\", \"NV\")\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"languagesAtSchool\", ArrayType(StringType()), True),\n",
        "    StructField(\"languagesAtWork\", ArrayType(StringType()), True),\n",
        "    StructField(\"currentState\", StringType(), True),\n",
        "    StructField(\"previousState\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "nfs2qLyF8KWD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Defines sample data as a list of tuples, where each tuple represents a row in the DataFrame.\n",
        "- Defines a schema with five fields: `name`, `languagesAtSchool`, `languagesAtWork`, `currentState`, and `previousState`.\n"
      ],
      "metadata": {
        "id": "q2vo_9s68MUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Creating DataFrame**:\n"
      ],
      "metadata": {
        "id": "oN8SZ7FN8N01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data=data, schema=schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AulkvMJi8P6l",
        "outputId": "f9eee01e-29f2-4029-8277-d092d070accb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- languagesAtSchool: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- languagesAtWork: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- currentState: string (nullable = true)\n",
            " |-- previousState: string (nullable = true)\n",
            "\n",
            "+----------------+------------------+---------------+------------+-------------+\n",
            "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
            "+----------------+------------------+---------------+------------+-------------+\n",
            "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
            "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
            "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
            "+----------------+------------------+---------------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Creates a DataFrame from the sample data and schema.\n",
        "- Prints the schema of the DataFrame.\n",
        "- Displays the content of the DataFrame.\n"
      ],
      "metadata": {
        "id": "sF2XjbwT8Rh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Exploding Array Column**:\n"
      ],
      "metadata": {
        "id": "uq-IAnVb8S_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "df.select(df.name, explode(df.languagesAtSchool)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2dEFhtL8Uqc",
        "outputId": "813aab32-2185-43a3-f7a1-9d2adc79ff39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------+\n",
            "|            name|   col|\n",
            "+----------------+------+\n",
            "|    James,,Smith|  Java|\n",
            "|    James,,Smith| Scala|\n",
            "|    James,,Smith|   C++|\n",
            "|   Michael,Rose,| Spark|\n",
            "|   Michael,Rose,|  Java|\n",
            "|   Michael,Rose,|   C++|\n",
            "|Robert,,Williams|CSharp|\n",
            "|Robert,,Williams|    VB|\n",
            "+----------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imports the `explode` function from `pyspark.sql.functions`.\n",
        "- Uses `explode` to transform each element of the `languagesAtSchool` array column into a separate row, along with the corresponding `name`.\n"
      ],
      "metadata": {
        "id": "4XfiMN_r8WMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. **Splitting String Column into Array**:\n"
      ],
      "metadata": {
        "id": "Oq-mF-cO8Xgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split\n",
        "\n",
        "df.select(split(df.name, \",\").alias(\"nameAsArray\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjeYfldS8ZhU",
        "outputId": "95ec4cc6-eebb-4a45-e288-d098fd73a7b0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|         nameAsArray|\n",
            "+--------------------+\n",
            "|    [James, , Smith]|\n",
            "|   [Michael, Rose, ]|\n",
            "|[Robert, , Williams]|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imports the `split` function from `pyspark.sql.functions`.\n",
        "- Splits the `name` column by commas into an array of strings and displays the result as a new column `nameAsArray`.\n"
      ],
      "metadata": {
        "id": "nfi9f2VO8b74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. **Combining Columns into Array**:\n"
      ],
      "metadata": {
        "id": "5kPxLe5o8ea9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array\n",
        "\n",
        "df.select(df.name, array(df.currentState, df.previousState).alias(\"States\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsHPUFff8gN6",
        "outputId": "542f0cb1-08f2-4a49-b297-60f7f70c8fd2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------+\n",
            "|            name|  States|\n",
            "+----------------+--------+\n",
            "|    James,,Smith|[OH, CA]|\n",
            "|   Michael,Rose,|[NY, NJ]|\n",
            "|Robert,,Williams|[UT, NV]|\n",
            "+----------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Imports the `array` function from `pyspark.sql.functions`.\n",
        "- Combines `currentState` and `previousState` columns into a single array column named `States`.\n"
      ],
      "metadata": {
        "id": "rfbYxlci8iFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. **Checking for Element in Array**:\n"
      ],
      "metadata": {
        "id": "9J-pTAo98jhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array_contains\n",
        "\n",
        "df.select(df.name, array_contains(df.languagesAtSchool, \"Java\").alias(\"array_contains\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5G4QC4H8lRm",
        "outputId": "d51b071c-8b3f-489f-f97c-4c5215d0b505"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------------+\n",
            "|            name|array_contains|\n",
            "+----------------+--------------+\n",
            "|    James,,Smith|          true|\n",
            "|   Michael,Rose,|          true|\n",
            "|Robert,,Williams|         false|\n",
            "+----------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - Imports the `array_contains` function from `pyspark.sql.functions`.\n",
        "   - Checks if the `languagesAtSchool` array contains the element \"Java\" and displays the result as a new column `array_contains`."
      ],
      "metadata": {
        "id": "esQBdcfo8nEZ"
      }
    }
  ]
}