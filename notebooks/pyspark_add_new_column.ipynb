{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. **Install PySpark**"
      ],
      "metadata": {
        "id": "TJKjcexez9Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc51oRbw0MkQ",
        "outputId": "7c8035ba-eb05-447b-9aba-78054dae1e24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=639a7efe57abc3a225f289bf6ff8721765883f739e7e285c1689caa513c9279b\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Initialize Spark session**:\n"
      ],
      "metadata": {
        "id": "CPYkpBcZJmUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
      ],
      "metadata": {
        "id": "QCZOsT9aJopk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Initializes a Spark session with the application name 'SparkByExamples.com'.\n"
      ],
      "metadata": {
        "id": "49GX0JcRJsAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. **Create DataFrame**:\n"
      ],
      "metadata": {
        "id": "8J4XsaTjJtV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('James','Smith','M',3000), ('Anna','Rose','F',4100), ('Robert','Williams','M',6200)]\n",
        "columns = [\"firstname\", \"lastname\", \"gender\", \"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema=columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvDBeEEEJ0sr",
        "outputId": "ea2f04e0-9d6d-4abe-8f3c-e5a3d842b5a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------+------+\n",
            "|firstname|lastname|gender|salary|\n",
            "+---------+--------+------+------+\n",
            "|    James|   Smith|     M|  3000|\n",
            "|     Anna|    Rose|     F|  4100|\n",
            "|   Robert|Williams|     M|  6200|\n",
            "+---------+--------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a DataFrame from the sample data and schema, then displays it.\n"
      ],
      "metadata": {
        "id": "T2JVH3xuJ4dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. **Check if a column exists**:\n"
      ],
      "metadata": {
        "id": "QgN_8_zyJ6IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'salary1' not in df.columns:\n",
        "    print(\"Column 'salary1' does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VkqGtsGKCib",
        "outputId": "b483a2f0-c06a-4316-9c62-d5262bd91e27"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'salary1' does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Checks if the column 'salary1' exists in the DataFrame columns.\n"
      ],
      "metadata": {
        "id": "rBEaNyxjKEgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. **Add a constant column**:\n"
      ],
      "metadata": {
        "id": "PBVBOvZBKHwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "df.withColumn(\"bonus_percent\", lit(0.3)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctj4JnVPKKBV",
        "outputId": "6dd4cacd-0427-4c49-ab4a-a484ac084c54"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------+------+-------------+\n",
            "|firstname|lastname|gender|salary|bonus_percent|\n",
            "+---------+--------+------+------+-------------+\n",
            "|    James|   Smith|     M|  3000|          0.3|\n",
            "|     Anna|    Rose|     F|  4100|          0.3|\n",
            "|   Robert|Williams|     M|  6200|          0.3|\n",
            "+---------+--------+------+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Adds a new column 'bonus_percent' with a constant value of 0.3.\n"
      ],
      "metadata": {
        "id": "vsKVROV7KMRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "`lit` is a function from the pyspark.sql.functions module that is used to create a column with a literal (constant) value. This can be useful when you want to add a new column to a DataFrame with a constant value or when you need to use a constant value in an expression."
      ],
      "metadata": {
        "id": "JhLwcYVCMTDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. **Add a column from an existing column**:\n"
      ],
      "metadata": {
        "id": "1e_IwLzsKOBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"bonus_amount\", df.salary * 0.3).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Jnv00rKRJr",
        "outputId": "2c796aba-1fa5-45b3-8afb-3223cf78db04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------+------+------------+\n",
            "|firstname|lastname|gender|salary|bonus_amount|\n",
            "+---------+--------+------+------+------------+\n",
            "|    James|   Smith|     M|  3000|       900.0|\n",
            "|     Anna|    Rose|     F|  4100|      1230.0|\n",
            "|   Robert|Williams|     M|  6200|      1860.0|\n",
            "+---------+--------+------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Adds a new column 'bonus_amount' by multiplying the 'salary' column by 0.3.\n"
      ],
      "metadata": {
        "id": "ySjUxUrtKTbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. **Add a column by concatenating existing columns**:\n"
      ],
      "metadata": {
        "id": "cnNuQwkpKVEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws\n",
        "df.withColumn(\"name\", concat_ws(\" \", \"firstname\", \"lastname\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Effb9KfKXmy",
        "outputId": "232f701c-0736-41d6-999a-a7e3462d33c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------+------+---------------+\n",
            "|firstname|lastname|gender|salary|           name|\n",
            "+---------+--------+------+------+---------------+\n",
            "|    James|   Smith|     M|  3000|    James Smith|\n",
            "|     Anna|    Rose|     F|  4100|      Anna Rose|\n",
            "|   Robert|Williams|     M|  6200|Robert Williams|\n",
            "+---------+--------+------+------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Adds a new column 'name' by concatenating 'firstname' and 'lastname' columns.\n"
      ],
      "metadata": {
        "id": "y35DZZunKZbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `concat_ws` function in PySpark is used to concatenate multiple columns into a single column, with a specified separator between each value. The \"ws\" stands for \"with separator\". This function is particularly useful when you need to combine multiple columns into one string column with a specific delimiter."
      ],
      "metadata": {
        "id": "vNt-dS2BMyX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. **Add the current date column**:\n"
      ],
      "metadata": {
        "id": "maogfy5HKbmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_date\n",
        "df.withColumn(\"current_date\", current_date()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quphdZsWKdlX",
        "outputId": "2cfeb619-c881-48b2-a6a6-e994b8043888"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------+------+------------+\n",
            "|firstname|lastname|gender|salary|current_date|\n",
            "+---------+--------+------+------+------------+\n",
            "|    James|   Smith|     M|  3000|  2024-07-15|\n",
            "|     Anna|    Rose|     F|  4100|  2024-07-15|\n",
            "|   Robert|Williams|     M|  6200|  2024-07-15|\n",
            "+---------+--------+------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Adds a new column 'current_date' with the current date.\n"
      ],
      "metadata": {
        "id": "zmqE0ikIKib_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `current_date` function in PySpark is used to retrieve the current date as a DateType column. This function is particularly useful when you need to add the current date to a DataFrame or use the current date in date-based calculations and comparisons."
      ],
      "metadata": {
        "id": "7rn9FG61Oeip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. **Add a column based on condition**:\n"
      ],
      "metadata": {
        "id": "0ovWRhmuKjqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "df.withColumn(\"grade\",\n",
        "    when((df.salary < 4000), lit(\"A\"))\n",
        "    .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\"))\n",
        "    .otherwise(lit(\"C\"))\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh2H1c_sKl6i",
        "outputId": "440a2e21-448b-4bac-989c-29d4237880c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------+------+-----+\n",
            "|firstname|lastname|gender|salary|grade|\n",
            "+---------+--------+------+------+-----+\n",
            "|    James|   Smith|     M|  3000|    A|\n",
            "|     Anna|    Rose|     F|  4100|    B|\n",
            "|   Robert|Williams|     M|  6200|    C|\n",
            "+---------+--------+------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Adds a new column 'grade' based on the salary value:\n",
        "   - 'A' if salary is less than 4000.\n",
        "   - 'B' if salary is between 4000 and 5000.\n",
        "   - 'C' otherwise.\n"
      ],
      "metadata": {
        "id": "1Fg1pqY1KpfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `when` function in PySpark is used to apply conditional logic to DataFrame columns. It's similar to the CASE WHEN statement in SQL. The when function allows you to create a new column based on conditions you define, and you can chain multiple when calls together to handle multiple conditions. If none of the conditions are met, you can use the otherwise method to provide a default value."
      ],
      "metadata": {
        "id": "vZ-50C9NQejL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. **Add columns using select**:\n"
      ],
      "metadata": {
        "id": "wMmG-EMhKq52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"firstname\", \"salary\", lit(0.3).alias(\"bonus\")).show()\n",
        "df.select(\"firstname\", \"salary\", (df.salary * 0.3).alias(\"bonus_amount\")).show()\n",
        "df.select(\"firstname\", \"salary\", current_date().alias(\"today_date\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOLbFr4gKupD",
        "outputId": "eef24926-b2bd-435e-97a2-d4e4bb2758c9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+-----+\n",
            "|firstname|salary|bonus|\n",
            "+---------+------+-----+\n",
            "|    James|  3000|  0.3|\n",
            "|     Anna|  4100|  0.3|\n",
            "|   Robert|  6200|  0.3|\n",
            "+---------+------+-----+\n",
            "\n",
            "+---------+------+------------+\n",
            "|firstname|salary|bonus_amount|\n",
            "+---------+------+------------+\n",
            "|    James|  3000|       900.0|\n",
            "|     Anna|  4100|      1230.0|\n",
            "|   Robert|  6200|      1860.0|\n",
            "+---------+------+------------+\n",
            "\n",
            "+---------+------+----------+\n",
            "|firstname|salary|today_date|\n",
            "+---------+------+----------+\n",
            "|    James|  3000|2024-07-15|\n",
            "|     Anna|  4100|2024-07-15|\n",
            "|   Robert|  6200|2024-07-15|\n",
            "+---------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   Adds new columns 'bonus', 'bonus_amount', and 'today_date' using the `select` method.\n"
      ],
      "metadata": {
        "id": "I2JghuXyKwip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. **Add columns using SQL**:\n"
      ],
      "metadata": {
        "id": "DUvOkeS_KyMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"PER\")\n",
        "\n",
        "spark.sql(\"SELECT firstname, salary, '0.3' AS bonus FROM PER\").show()\n",
        "spark.sql(\"SELECT firstname, salary, salary * 0.3 AS bonus_amount FROM PER\").show()\n",
        "spark.sql(\"SELECT firstname, salary, current_date() AS today_date FROM PER\").show()\n",
        "\n",
        "spark.sql(\n",
        "    \"SELECT firstname, salary, \" +\n",
        "    \"CASE WHEN salary < 4000 THEN 'A' \" +\n",
        "    \"WHEN salary >= 4000 AND salary <= 5000 THEN 'B' \" +\n",
        "    \"ELSE 'C' END AS grade \" +\n",
        "    \"FROM PER\"\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jono7iEK1Jh",
        "outputId": "485c29da-0c0e-4ee7-9aaf-116b55dea30e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+-----+\n",
            "|firstname|salary|bonus|\n",
            "+---------+------+-----+\n",
            "|    James|  3000|  0.3|\n",
            "|     Anna|  4100|  0.3|\n",
            "|   Robert|  6200|  0.3|\n",
            "+---------+------+-----+\n",
            "\n",
            "+---------+------+------------+\n",
            "|firstname|salary|bonus_amount|\n",
            "+---------+------+------------+\n",
            "|    James|  3000|       900.0|\n",
            "|     Anna|  4100|      1230.0|\n",
            "|   Robert|  6200|      1860.0|\n",
            "+---------+------+------------+\n",
            "\n",
            "+---------+------+----------+\n",
            "|firstname|salary|today_date|\n",
            "+---------+------+----------+\n",
            "|    James|  3000|2024-07-15|\n",
            "|     Anna|  4100|2024-07-15|\n",
            "|   Robert|  6200|2024-07-15|\n",
            "+---------+------+----------+\n",
            "\n",
            "+---------+------+-----+\n",
            "|firstname|salary|grade|\n",
            "+---------+------+-----+\n",
            "|    James|  3000|    A|\n",
            "|     Anna|  4100|    B|\n",
            "|   Robert|  6200|    C|\n",
            "+---------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uses SQL to add new columns 'bonus', 'bonus_amount', 'today_date', and 'grade' based on conditions.\n"
      ],
      "metadata": {
        "id": "QlHSpsYRK4dE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `createOrReplaceTempView` method in PySpark is used to create a temporary view from a DataFrame. This temporary view can be queried using SQL syntax within the same Spark session. This method is particularly useful when you want to perform SQL-like operations on a DataFrame.\n",
        "\n",
        "**Usage of `createOrReplaceTempView`**\n",
        "\n",
        "1. **Syntax**:\n",
        "   ```python\n",
        "   df.createOrReplaceTempView(\"view_name\")\n",
        "   ```\n",
        "   - `df`: The DataFrame you want to create the view from.\n",
        "   - `\"view_name\"`: The name of the temporary view.\n",
        "\n",
        "2. **Purpose**:\n",
        "   - Allows you to query the DataFrame using SQL.\n",
        "   - Useful for performing complex SQL operations and aggregations.\n",
        "   - Integrates with other Spark SQL functionalities.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Here's an example demonstrating how to use `createOrReplaceTempView` and perform SQL queries on a DataFrame:\n"
      ],
      "metadata": {
        "id": "MV5WUDW3SP8f"
      }
    }
  ]
}