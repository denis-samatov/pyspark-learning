{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "C8R4lbil3Jpw",
        "jeKygNvu3ML6",
        "QUpqAldR3P2A",
        "ku8BsDK73ToE",
        "iwyD3ptY3W_Z",
        "QOum43hY3awz",
        "qleaqCiT7-Na",
        "I66rxUsU3d67",
        "z7xl6wEj3hBS",
        "RZF3OQLO3jwn",
        "Nm61YvJ63m-7",
        "pQ54i0hQ3ptC",
        "lUIqy1dP3s6M"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark DataFrame Operations in Google Colab\n",
        "\n",
        "# This notebook demonstrates various methods to convert a column from a PySpark DataFrame into a Python list."
      ],
      "metadata": {
        "id": "-5HkoCpk3ChE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install PySpark"
      ],
      "metadata": {
        "id": "C8R4lbil3Jpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment and run the following line in Google Colab to install PySpark.\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7Xyj3Oe3IoM",
        "outputId": "42e10b5a-a9a8-4219-c46e-1d3971f5d0b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=c5034a9eddb111889c795dd6e34e615730f8c3d87f235f32b6366351d08424c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import the necessary libraries\n"
      ],
      "metadata": {
        "id": "jeKygNvu3ML6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "vfH1ZNos3NUX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Initialize the Spark session"
      ],
      "metadata": {
        "id": "QUpqAldR3P2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `SparkSession.builder` method is used to configure and initialize a Spark session, which is the entry point to programming with Spark. Let's break down the parameters used in this method:\n",
        "\n",
        "1. **`master(\"local[1]\")`**:\n",
        "   - **Purpose**: Specifies the master URL for the Spark application.\n",
        "   - **Meaning**:\n",
        "     - `\"local\"`: Indicates that the Spark application will run locally on a single machine.\n",
        "     - `\"[1]\"`: Specifies that Spark should use only one thread for execution. This is useful for testing and development purposes. You can change the number of threads to `\"[n]\"` where `n` is the number of threads you want to use. For example, `\"local[4]\"` would use four threads.\n",
        "\n",
        "2. **`appName('SparkByExamples.com')`**:\n",
        "   - **Purpose**: Sets the name of the Spark application.\n",
        "   - **Meaning**:\n",
        "     - `'SparkByExamples.com'`: This is an arbitrary name for the application. It helps identify your application in the Spark UI or logs. You can name it anything that helps you recognize your application.\n",
        "\n",
        "3. **`getOrCreate()`**:\n",
        "   - **Purpose**: This method either retrieves an existing Spark session or, if none exists, creates a new one.\n",
        "   - **Meaning**:\n",
        "     - Ensures that you don't accidentally create multiple Spark sessions in your application. If a session already exists, it will return that session instead of creating a new one.\n"
      ],
      "metadata": {
        "id": "TwyoBOxn7DaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()"
      ],
      "metadata": {
        "id": "vuNoZUD73QkP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Create a DataFrame"
      ],
      "metadata": {
        "id": "ku8BsDK73ToE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"James\", \"Smith\", \"USA\", \"CA\"),\n",
        "    (\"Michael\", \"Rose\", \"USA\", \"NY\"),\n",
        "    (\"Robert\", \"Williams\", \"USA\", \"CA\"),\n",
        "    (\"Maria\", \"Jones\", \"USA\", \"FL\")\n",
        "]\n",
        "columns = [\"firstname\", \"lastname\", \"country\", \"state\"]\n",
        "df = spark.createDataFrame(data=data, schema=columns)"
      ],
      "metadata": {
        "id": "mDEq6lWG3UWQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Show the DataFrame"
      ],
      "metadata": {
        "id": "iwyD3ptY3W_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hjXfd3f7aqu",
        "outputId": "75e3c487-7f91-4c36-b5c2-47b0347e70f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|    James|   Smith|    USA|   CA|\n",
            "|  Michael|    Rose|    USA|   NY|\n",
            "|   Robert|Williams|    USA|   CA|\n",
            "|    Maria|   Jones|    USA|   FL|\n",
            "+---------+--------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F40LkPc-3Xf6",
        "outputId": "3bf167e9-d18a-4de3-9c1d-3ccb51a43dda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(firstname='James', lastname='Smith', country='USA', state='CA'),\n",
              " Row(firstname='Michael', lastname='Rose', country='USA', state='NY'),\n",
              " Row(firstname='Robert', lastname='Williams', country='USA', state='CA'),\n",
              " Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Convert Column to List Using RDD map"
      ],
      "metadata": {
        "id": "QOum43hY3awz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrames can be converted to Resilient Distributed Datasets (RDDs), which are the fundamental data structures of Spark. The `rdd` property of a DataFrame returns the underlying RDD. The `map` function is an RDD transformation that applies a given function to each element of the RDD and returns a new RDD with the results. Let's break down the statement `states1 = df.rdd.map(lambda x: x[3]).collect()`:\n",
        "\n",
        "**Breaking Down the Code**\n",
        "\n",
        "1. **`df.rdd`**:\n",
        "   - **Purpose**: Converts the DataFrame `df` to its underlying RDD.\n",
        "   - **Meaning**:\n",
        "     - `df` is your DataFrame. The `rdd` property accesses the RDD representation of this DataFrame.\n",
        "   \n",
        "2. **`.map(lambda x: x[3])`**:\n",
        "   - **Purpose**: Applies a function to each element of the RDD.\n",
        "   - **Meaning**:\n",
        "     - `map` is a transformation operation in Spark that applies the given function (here, a lambda function) to each element of the RDD.\n",
        "     - `lambda x: x[3]` is a lambda function that takes each row (`x`) of the RDD and extracts the element at index `3` (the fourth column in the DataFrame).\n",
        "   \n",
        "3. **`.collect()`**:\n",
        "   - **Purpose**: Collects the results of the RDD transformations back to the driver program.\n",
        "   - **Meaning**:\n",
        "     - `collect` is an action in Spark that gathers all elements of the RDD and returns them as a list to the driver program.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "The line `states1 = df.rdd.map(lambda x: x[3]).collect()`:\n",
        "\n",
        "- Converts the DataFrame to an RDD.\n",
        "- Maps each row of the RDD to its fourth column (state).\n",
        "- Collects the list of states back to the driver program.\n",
        "\n",
        "In essence, it extracts the \"state\" column from the DataFrame and returns it as a list."
      ],
      "metadata": {
        "id": "qleaqCiT7-Na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states1 = df.rdd.map(lambda x: x[3]).collect()"
      ],
      "metadata": {
        "id": "pwVVoPI63bav"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states1  # Output: ['CA', 'NY', 'CA', 'FL']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agS1iE-k7i6n",
        "outputId": "c29180b4-148c-4e13-cecb-e8835b95426b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CA', 'NY', 'CA', 'FL']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Remove Duplicates Using OrderedDict"
      ],
      "metadata": {
        "id": "I66rxUsU3d67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "res = list(OrderedDict.fromkeys(states1))\n",
        "\n",
        "print(res)  # Output: ['CA', 'NY', 'FL']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0V-VAYU3eic",
        "outputId": "6c24032b-1da5-497f-ce90-18b1840491bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CA', 'NY', 'FL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Convert Column to List Using DataFrame map"
      ],
      "metadata": {
        "id": "z7xl6wEj3hBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states2 = df.rdd.map(lambda x: x.state).collect()\n",
        "\n",
        "print(states2)  # Output: ['CA', 'NY', 'CA', 'FL']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oghLVYF3hkn",
        "outputId": "73e1accd-9f32-4a40-ebf5-1f468978cd59"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CA', 'NY', 'CA', 'FL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Convert Column to List Using select and collect"
      ],
      "metadata": {
        "id": "RZF3OQLO3jwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states3 = df.select(df.state).collect()\n",
        "print(states3)  # Output: [Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHl5pLNW3kYw",
        "outputId": "cfa4f51b-a478-4add-a62d-79d79cdc557d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Flatten the List of Rows"
      ],
      "metadata": {
        "id": "Nm61YvJ63m-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `flatMap` function in PySpark is an RDD transformation that maps each input element to an iterable (e.g., a list or another RDD) and then flattens the results. It applies the provided function to each element of the RDD and returns a new RDD by concatenating the results of the function.\n",
        "\n",
        "**Breaking Down the Code**\n",
        "\n",
        "1. **`df.select(df.state)`**:\n",
        "   - **Purpose**: Selects the `state` column from the DataFrame.\n",
        "   - **Meaning**:\n",
        "     - Creates a new DataFrame containing only the `state` column.\n",
        "\n",
        "2. **`.rdd`**:\n",
        "   - **Purpose**: Converts the DataFrame to its underlying RDD.\n",
        "   - **Meaning**:\n",
        "     - Each row in the new DataFrame (containing only the `state` column) becomes a Row object in the RDD.\n",
        "\n",
        "3. **`.flatMap(lambda x: x)`**:\n",
        "   - **Purpose**: Applies a function to each element of the RDD and flattens the results.\n",
        "   - **Meaning**:\n",
        "     - `flatMap` is a transformation that takes a function as an argument.\n",
        "     - `lambda x: x` is a lambda function that takes each Row object (`x`) and returns it as-is.\n",
        "     - Because the Row object contains just one element (the `state` value), `flatMap` effectively extracts and flattens this value.\n",
        "\n",
        "4. **`.collect()`**:\n",
        "   - **Purpose**: Collects the results of the RDD transformations back to the driver program.\n",
        "   - **Meaning**:\n",
        "     - `collect` is an action that gathers all elements of the RDD and returns them as a list to the driver program.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "The line `states4 = df.select(df.state).rdd.flatMap(lambda x: x).collect()`:\n",
        "\n",
        "- Selects the `state` column from the DataFrame, resulting in a new DataFrame.\n",
        "- Converts this new DataFrame to an RDD where each row is a Row object containing a single element (`state` value).\n",
        "- Uses `flatMap` with a lambda function that extracts the `state` value from each Row object.\n",
        "- Collects the list of state values back to the driver program.\n",
        "\n",
        "In essence, `flatMap` is used to flatten the Row objects and extract the `state` values into a single list."
      ],
      "metadata": {
        "id": "wvaVUP9P9z9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states4 = df.select(df.state).rdd.flatMap(lambda x: x).collect()\n",
        "print(states4)  # Output: ['CA', 'NY', 'CA', 'FL']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsRAx0-x3nbi",
        "outputId": "8b48963d-b9f2-4316-f41a-5d68115d6eac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CA', 'NY', 'CA', 'FL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Convert Column to List Using toPandas"
      ],
      "metadata": {
        "id": "pQ54i0hQ3ptC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states5 = df.select(df.state).toPandas()['state']\n",
        "states6 = list(states5)\n",
        "print(states6)  # Output: ['CA', 'NY', 'CA', 'FL']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4zpll0-3qW9",
        "outputId": "1a9846ed-8103-471b-f43d-c9cf06f4095f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CA', 'NY', 'CA', 'FL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 11: Convert Multiple Columns to List Using toPandas"
      ],
      "metadata": {
        "id": "lUIqy1dP3s6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pandDF = df.select(df.state, df.firstname).toPandas()\n",
        "print(list(pandDF['state']))      # Output: ['CA', 'NY', 'CA', 'FL']\n",
        "print(list(pandDF['firstname']))  # Output: ['James', 'Michael', 'Robert', 'Maria']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q28qYRX3tiq",
        "outputId": "d785e8c9-8670-4530-ecf7-9178060ac91d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CA', 'NY', 'CA', 'FL']\n",
            "['James', 'Michael', 'Robert', 'Maria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWqfPITh-EnL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}